{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -r requirements.txt\n",
        "import numpy as np\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from functools import partial\n",
        "from preprocessing import LABELS\n",
        "from time import time\n",
        "import random\n",
        "from preprocessing import get_mfccs\n",
        "from functools import partial"
      ],
      "metadata": {
        "tags": [],
        "cell_id": "f1ada0dd4064420c9a470dd2ab8336ae",
        "source_hash": "eefa9445",
        "execution_start": 1671096655771,
        "execution_millis": 4375,
        "deepnote_to_be_reexecuted": false,
        "deepnote_cell_type": "code",
        "id": "jYESf1ZvcBQT",
        "outputId": "db3d5371-eb65-48ee-b14a-cf7f50cfbde7"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.3.1 is available.\nYou should consider upgrading via the '/root/venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n\u001b[0m2022-12-15 09:30:58.498073: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2022-12-15 09:30:58.610317: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n2022-12-15 09:30:58.615485: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n2022-12-15 09:30:58.615499: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n2022-12-15 09:30:58.638711: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2022-12-15 09:30:59.252768: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n2022-12-15 09:30:59.252827: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n2022-12-15 09:30:59.252833: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = tf.data.Dataset.list_files(['./msc-test/go*','./msc-test/stop*' ])\n",
        "test_ds = tf.data.Dataset.list_files(['./msc-test/go*','./msc-test/stop*' ])\n",
        "val_ds = tf.data.Dataset.list_files(['./msc-val/go*','./msc-val/stop*' ])"
      ],
      "metadata": {
        "tags": [],
        "cell_id": "5922b1fbcf8446868ee901789f63a960",
        "source_hash": "b0de374e",
        "execution_start": 1671096660173,
        "execution_millis": 116,
        "deepnote_to_be_reexecuted": false,
        "deepnote_cell_type": "code",
        "id": "mw8ReIAIcBQV",
        "outputId": "b507ca8d-8e19-4f01-89cb-b162c85168e4"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "2022-12-15 09:31:00.147914: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n2022-12-15 09:31:00.147943: W tensorflow/stream_executor/cuda/cuda_driver.cc:263] failed call to cuInit: UNKNOWN ERROR (303)\n2022-12-15 09:31:00.147958: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (p-c40ae498-620e-4b09-a576-182e0c5bf3b3): /proc/driver/nvidia/version does not exist\n2022-12-15 09:31:00.148219: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Deterministic environment \n",
        "seed = 17\n",
        "os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
        "random.seed(seed)\n",
        "tf.random.set_seed(seed)\n",
        "np.random.seed(seed)"
      ],
      "metadata": {
        "tags": [],
        "cell_id": "9a3edbcd05e745b388dab089e1d91ac8",
        "source_hash": "25e93dd1",
        "execution_start": 1671096660286,
        "execution_millis": 0,
        "deepnote_to_be_reexecuted": false,
        "deepnote_cell_type": "code",
        "id": "vJ6lF4V8cBQV"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "TRAINING_ARGS = {\n",
        "    'batch_size': 20,\n",
        "    'epochs': 60,\n",
        "    'initial_learning_rate': 0.001,\n",
        "    'end_learning_rate': 1.e-5,\n",
        "}"
      ],
      "metadata": {
        "tags": [],
        "cell_id": "eb0b84405e44491ba1c30b0f7b75c334",
        "source_hash": "d145fac5",
        "execution_start": 1671096660287,
        "execution_millis": 0,
        "deepnote_to_be_reexecuted": false,
        "deepnote_cell_type": "code",
        "id": "zoQUZ7KmcBQV"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "MFCCS_ARGS = {\n",
        "    'downsampling_rate': 16000,\n",
        "    'frame_length_in_s': 0.032,\n",
        "    'frame_step_in_s': 0.016,\n",
        "    'num_mel_bins': 10,\n",
        "    'lower_frequency': 20,\n",
        "    'upper_frequency': 8000,\n",
        "    'num_coefficients': 10\n",
        "}"
      ],
      "metadata": {
        "tags": [],
        "cell_id": "b53f319ee11b4b61ac25136d9c654caa",
        "source_hash": "82b73890",
        "execution_start": 1671113235571,
        "execution_millis": 16575281,
        "deepnote_to_be_reexecuted": false,
        "deepnote_cell_type": "code",
        "id": "gSoOFqqocBQV"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting Preprocessing function\n",
        "get_mfccs_frozen = partial(get_mfccs, **MFCCS_ARGS)\n",
        "\n",
        "def resize_mfccs_(filename):\n",
        "    signal, label = get_mfccs_frozen(filename)\n",
        "    signal = tf.expand_dims(signal, -1)\n",
        "    #signal = tf.image.resize(signal, [32, 32])\n",
        "    label_id = tf.argmax(label == LABELS)\n",
        "    return signal, label_id"
      ],
      "metadata": {
        "tags": [],
        "cell_id": "3d3ebfa0a38848f3bd6b2109363ec2b5",
        "source_hash": "91ae6df3",
        "execution_start": 1671096660290,
        "execution_millis": 1,
        "deepnote_to_be_reexecuted": false,
        "deepnote_cell_type": "code",
        "id": "cQ86igpMcBQW"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Applying it to DS\n",
        "train_ds_ = train_ds.map(resize_mfccs_).batch(TRAINING_ARGS['batch_size']).cache()\n",
        "test_ds_ = test_ds.map(resize_mfccs_).batch(TRAINING_ARGS['batch_size'])\n",
        "val_ds_ = val_ds.map(resize_mfccs_).batch(TRAINING_ARGS['batch_size'])"
      ],
      "metadata": {
        "tags": [],
        "cell_id": "2d8703fa591e4138bb93df46f3775254",
        "source_hash": "b96c6846",
        "execution_start": 1671096660294,
        "execution_millis": 1601,
        "deepnote_to_be_reexecuted": false,
        "deepnote_cell_type": "code",
        "id": "mfO_eywJcBQW",
        "outputId": "f4b449a2-17cd-48d7-b72c-afb80e5153c7"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "2022-12-15 09:31:00.610585: W tensorflow_io/core/kernels/audio_video_mp3_kernels.cc:271] libmp3lame.so.0 or lame functions are not available\n2022-12-15 09:31:00.610787: I tensorflow_io/core/kernels/cpu_check.cc:128] Your CPU supports instructions that this TensorFlow IO binary was not compiled to use: AVX2 AVX512F FMA\nWARNING:tensorflow:Using a while_loop for converting IO>AudioResample cause there is no registered converter for this op.\n2022-12-15 09:31:01.011348: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n2022-12-15 09:31:01.012834: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n2022-12-15 09:31:01.013015: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\nWARNING:tensorflow:Using a while_loop for converting IO>AudioResample cause there is no registered converter for this op.\n2022-12-15 09:31:01.382170: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n2022-12-15 09:31:01.383606: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n2022-12-15 09:31:01.383765: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\nWARNING:tensorflow:Using a while_loop for converting IO>AudioResample cause there is no registered converter for this op.\n2022-12-15 09:31:01.708838: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n2022-12-15 09:31:01.710353: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n2022-12-15 09:31:01.710544: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "for example_tensor,example_label in train_ds_.take(1):\n",
        "    example_batch = example_tensor\n",
        "example_batch.shape"
      ],
      "metadata": {
        "tags": [],
        "cell_id": "9ea15af216394e8f9de5c51aa8eb3070",
        "source_hash": "a92bef85",
        "execution_start": 1671096661944,
        "execution_millis": 331,
        "deepnote_to_be_reexecuted": false,
        "deepnote_cell_type": "code",
        "id": "AQdG9gl5cBQW",
        "outputId": "ff8b7190-4d0b-4fd3-c9a5-c361c0accf57"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "2022-12-15 09:31:02.258314: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
          "output_type": "stream"
        },
        {
          "output_type": "execute_result",
          "execution_count": 8,
          "data": {
            "text/plain": "TensorShape([20, 61, 10, 1])"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Input(shape=example_batch.shape[1:]),\n",
        "    tf.keras.layers.Conv2D(filters=128, kernel_size=[3, 3], strides=[2, 2],\n",
        "        use_bias=False, padding='valid'),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.ReLU(),\n",
        "    tf.keras.layers.Dropout(rate=0.1),\n",
        "    tf.keras.layers.DepthwiseConv2D(kernel_size=[3, 3], strides=[1, 1],\n",
        "        use_bias=True, padding='same'),\n",
        "    tf.keras.layers.Conv2D(filters=32, kernel_size=[1, 1], strides=[1, 1],\n",
        "        use_bias=False, padding='valid'),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.ReLU(),\n",
        "    tf.keras.layers.GlobalMaxPool2D(),\n",
        "    tf.keras.layers.Dense(units=len(LABELS)),\n",
        "    tf.keras.layers.Softmax()\n",
        "])\n",
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
        "\n",
        "initial_learning_rate = TRAINING_ARGS['initial_learning_rate']\n",
        "end_learning_rate = TRAINING_ARGS['end_learning_rate']\n",
        "\n",
        "linear_decay = tf.keras.optimizers.schedules.PolynomialDecay(\n",
        "    initial_learning_rate=initial_learning_rate,\n",
        "    end_learning_rate=end_learning_rate,\n",
        "    decay_steps=len(train_ds_) * TRAINING_ARGS['epochs'],\n",
        ")\n",
        "optimizer = tf.optimizers.Adam(learning_rate=linear_decay)\n",
        "metrics = [tf.metrics.SparseCategoricalAccuracy()]\n",
        "\n",
        "model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
        "model.summary()"
      ],
      "metadata": {
        "tags": [],
        "cell_id": "205f511cbdef408693bea789681f354a",
        "source_hash": "54dc634d",
        "execution_start": 1671096662289,
        "execution_millis": 136,
        "deepnote_to_be_reexecuted": false,
        "deepnote_cell_type": "code",
        "id": "qUrbQzNmcBQW",
        "outputId": "bb1fdce9-6a2d-4b4c-fc32-d6bf3c6ea86c"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Model: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n conv2d (Conv2D)             (None, 30, 4, 128)        1152      \n                                                                 \n batch_normalization (BatchN  (None, 30, 4, 128)       512       \n ormalization)                                                   \n                                                                 \n re_lu (ReLU)                (None, 30, 4, 128)        0         \n                                                                 \n dropout (Dropout)           (None, 30, 4, 128)        0         \n                                                                 \n depthwise_conv2d (Depthwise  (None, 30, 4, 128)       1280      \n Conv2D)                                                         \n                                                                 \n conv2d_1 (Conv2D)           (None, 30, 4, 32)         4096      \n                                                                 \n batch_normalization_1 (Batc  (None, 30, 4, 32)        128       \n hNormalization)                                                 \n                                                                 \n re_lu_1 (ReLU)              (None, 30, 4, 32)         0         \n                                                                 \n global_max_pooling2d (Globa  (None, 32)               0         \n lMaxPooling2D)                                                  \n                                                                 \n dense (Dense)               (None, 2)                 66        \n                                                                 \n softmax (Softmax)           (None, 2)                 0         \n                                                                 \n=================================================================\nTotal params: 7,234\nTrainable params: 6,914\nNon-trainable params: 320\n_________________________________________________________________\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "timestamp = time()\n",
        "history = model.fit(\n",
        "    train_ds_,\n",
        "    epochs=TRAINING_ARGS['epochs'],\n",
        "    validation_data=val_ds_,\n",
        "    callbacks=[\n",
        "        tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "tags": [],
        "cell_id": "03f4be2b8045498eb2ecf3c8a2baa70e",
        "source_hash": "5e65dec2",
        "execution_start": 1671096662467,
        "execution_millis": 58440,
        "deepnote_to_be_reexecuted": false,
        "deepnote_cell_type": "code",
        "id": "uOv85rHWcBQX",
        "outputId": "67de070b-484d-4eb4-b7e3-172bf666fa33"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Epoch 1/60\n10/10 [==============================] - 2s 160ms/step - loss: 1.7681 - sparse_categorical_accuracy: 0.5700 - val_loss: 0.6850 - val_sparse_categorical_accuracy: 0.5050\nEpoch 2/60\n10/10 [==============================] - 1s 113ms/step - loss: 0.6016 - sparse_categorical_accuracy: 0.8000 - val_loss: 0.6745 - val_sparse_categorical_accuracy: 0.5950\nEpoch 3/60\n10/10 [==============================] - 1s 109ms/step - loss: 0.3519 - sparse_categorical_accuracy: 0.8550 - val_loss: 0.6631 - val_sparse_categorical_accuracy: 0.7650\nEpoch 4/60\n10/10 [==============================] - 1s 109ms/step - loss: 0.2579 - sparse_categorical_accuracy: 0.9100 - val_loss: 0.6530 - val_sparse_categorical_accuracy: 0.8250\nEpoch 5/60\n10/10 [==============================] - 1s 110ms/step - loss: 0.2173 - sparse_categorical_accuracy: 0.9200 - val_loss: 0.6436 - val_sparse_categorical_accuracy: 0.8200\nEpoch 6/60\n10/10 [==============================] - 1s 110ms/step - loss: 0.1674 - sparse_categorical_accuracy: 0.9550 - val_loss: 0.6335 - val_sparse_categorical_accuracy: 0.8750\nEpoch 7/60\n10/10 [==============================] - 1s 111ms/step - loss: 0.1755 - sparse_categorical_accuracy: 0.9500 - val_loss: 0.6200 - val_sparse_categorical_accuracy: 0.8750\nEpoch 8/60\n10/10 [==============================] - 1s 110ms/step - loss: 0.1399 - sparse_categorical_accuracy: 0.9550 - val_loss: 0.6082 - val_sparse_categorical_accuracy: 0.8900\nEpoch 9/60\n10/10 [==============================] - 1s 110ms/step - loss: 0.1324 - sparse_categorical_accuracy: 0.9500 - val_loss: 0.5960 - val_sparse_categorical_accuracy: 0.8800\nEpoch 10/60\n10/10 [==============================] - 1s 110ms/step - loss: 0.1159 - sparse_categorical_accuracy: 0.9500 - val_loss: 0.5815 - val_sparse_categorical_accuracy: 0.9100\nEpoch 11/60\n10/10 [==============================] - 1s 110ms/step - loss: 0.0939 - sparse_categorical_accuracy: 0.9750 - val_loss: 0.5711 - val_sparse_categorical_accuracy: 0.8750\nEpoch 12/60\n10/10 [==============================] - 1s 110ms/step - loss: 0.0934 - sparse_categorical_accuracy: 0.9700 - val_loss: 0.5594 - val_sparse_categorical_accuracy: 0.8750\nEpoch 13/60\n10/10 [==============================] - 1s 102ms/step - loss: 0.0767 - sparse_categorical_accuracy: 0.9800 - val_loss: 0.5440 - val_sparse_categorical_accuracy: 0.9150\nEpoch 14/60\n10/10 [==============================] - 1s 102ms/step - loss: 0.0871 - sparse_categorical_accuracy: 0.9650 - val_loss: 0.5303 - val_sparse_categorical_accuracy: 0.8850\nEpoch 15/60\n10/10 [==============================] - 1s 102ms/step - loss: 0.0629 - sparse_categorical_accuracy: 0.9800 - val_loss: 0.5129 - val_sparse_categorical_accuracy: 0.9200\nEpoch 16/60\n10/10 [==============================] - 1s 110ms/step - loss: 0.0637 - sparse_categorical_accuracy: 0.9850 - val_loss: 0.4966 - val_sparse_categorical_accuracy: 0.9300\nEpoch 17/60\n10/10 [==============================] - 1s 115ms/step - loss: 0.0593 - sparse_categorical_accuracy: 0.9800 - val_loss: 0.4813 - val_sparse_categorical_accuracy: 0.9300\nEpoch 18/60\n10/10 [==============================] - 1s 111ms/step - loss: 0.0489 - sparse_categorical_accuracy: 0.9950 - val_loss: 0.4662 - val_sparse_categorical_accuracy: 0.9250\nEpoch 19/60\n10/10 [==============================] - 1s 108ms/step - loss: 0.0429 - sparse_categorical_accuracy: 0.9950 - val_loss: 0.4500 - val_sparse_categorical_accuracy: 0.9200\nEpoch 20/60\n10/10 [==============================] - 1s 111ms/step - loss: 0.0512 - sparse_categorical_accuracy: 0.9900 - val_loss: 0.4342 - val_sparse_categorical_accuracy: 0.9100\nEpoch 21/60\n10/10 [==============================] - 1s 111ms/step - loss: 0.0474 - sparse_categorical_accuracy: 0.9900 - val_loss: 0.4116 - val_sparse_categorical_accuracy: 0.9350\nEpoch 22/60\n10/10 [==============================] - 1s 110ms/step - loss: 0.0476 - sparse_categorical_accuracy: 0.9900 - val_loss: 0.3921 - val_sparse_categorical_accuracy: 0.9400\nEpoch 23/60\n10/10 [==============================] - 1s 110ms/step - loss: 0.0341 - sparse_categorical_accuracy: 0.9950 - val_loss: 0.3741 - val_sparse_categorical_accuracy: 0.9400\nEpoch 24/60\n10/10 [==============================] - 1s 110ms/step - loss: 0.0394 - sparse_categorical_accuracy: 0.9900 - val_loss: 0.3565 - val_sparse_categorical_accuracy: 0.9350\nEpoch 25/60\n10/10 [==============================] - 1s 110ms/step - loss: 0.0340 - sparse_categorical_accuracy: 0.9950 - val_loss: 0.3410 - val_sparse_categorical_accuracy: 0.9300\nEpoch 26/60\n10/10 [==============================] - 1s 110ms/step - loss: 0.0413 - sparse_categorical_accuracy: 0.9900 - val_loss: 0.3253 - val_sparse_categorical_accuracy: 0.9300\nEpoch 27/60\n10/10 [==============================] - 1s 111ms/step - loss: 0.0285 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.3070 - val_sparse_categorical_accuracy: 0.9300\nEpoch 28/60\n10/10 [==============================] - 1s 102ms/step - loss: 0.0278 - sparse_categorical_accuracy: 0.9950 - val_loss: 0.2935 - val_sparse_categorical_accuracy: 0.9350\nEpoch 29/60\n10/10 [==============================] - 1s 102ms/step - loss: 0.0310 - sparse_categorical_accuracy: 0.9950 - val_loss: 0.2825 - val_sparse_categorical_accuracy: 0.9400\nEpoch 30/60\n10/10 [==============================] - 1s 101ms/step - loss: 0.0212 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.2742 - val_sparse_categorical_accuracy: 0.9350\nEpoch 31/60\n10/10 [==============================] - 1s 111ms/step - loss: 0.0225 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.2656 - val_sparse_categorical_accuracy: 0.9350\nEpoch 32/60\n10/10 [==============================] - 1s 111ms/step - loss: 0.0233 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.2536 - val_sparse_categorical_accuracy: 0.9400\nEpoch 33/60\n10/10 [==============================] - 1s 115ms/step - loss: 0.0217 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.2429 - val_sparse_categorical_accuracy: 0.9400\nEpoch 34/60\n10/10 [==============================] - 1s 111ms/step - loss: 0.0252 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.2363 - val_sparse_categorical_accuracy: 0.9400\nEpoch 35/60\n10/10 [==============================] - 1s 110ms/step - loss: 0.0189 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.2296 - val_sparse_categorical_accuracy: 0.9400\nEpoch 36/60\n10/10 [==============================] - 1s 110ms/step - loss: 0.0249 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.2220 - val_sparse_categorical_accuracy: 0.9400\nEpoch 37/60\n10/10 [==============================] - 1s 110ms/step - loss: 0.0166 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.2157 - val_sparse_categorical_accuracy: 0.9400\nEpoch 38/60\n10/10 [==============================] - 1s 110ms/step - loss: 0.0207 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.2087 - val_sparse_categorical_accuracy: 0.9400\nEpoch 39/60\n10/10 [==============================] - 1s 102ms/step - loss: 0.0180 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.2035 - val_sparse_categorical_accuracy: 0.9450\nEpoch 40/60\n10/10 [==============================] - 1s 111ms/step - loss: 0.0174 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.1990 - val_sparse_categorical_accuracy: 0.9450\nEpoch 41/60\n10/10 [==============================] - 1s 112ms/step - loss: 0.0183 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.1964 - val_sparse_categorical_accuracy: 0.9450\nEpoch 42/60\n10/10 [==============================] - 1s 110ms/step - loss: 0.0154 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.1943 - val_sparse_categorical_accuracy: 0.9450\nEpoch 43/60\n10/10 [==============================] - 1s 111ms/step - loss: 0.0179 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.1922 - val_sparse_categorical_accuracy: 0.9450\nEpoch 44/60\n10/10 [==============================] - 1s 110ms/step - loss: 0.0196 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.1883 - val_sparse_categorical_accuracy: 0.9500\nEpoch 45/60\n10/10 [==============================] - 1s 111ms/step - loss: 0.0172 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.1849 - val_sparse_categorical_accuracy: 0.9550\nEpoch 46/60\n10/10 [==============================] - 1s 111ms/step - loss: 0.0170 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.1828 - val_sparse_categorical_accuracy: 0.9550\nEpoch 47/60\n10/10 [==============================] - 1s 111ms/step - loss: 0.0159 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.1822 - val_sparse_categorical_accuracy: 0.9500\nEpoch 48/60\n10/10 [==============================] - 1s 111ms/step - loss: 0.0161 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.1810 - val_sparse_categorical_accuracy: 0.9500\nEpoch 49/60\n10/10 [==============================] - 1s 112ms/step - loss: 0.0136 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.1801 - val_sparse_categorical_accuracy: 0.9500\nEpoch 50/60\n10/10 [==============================] - 1s 111ms/step - loss: 0.0199 - sparse_categorical_accuracy: 0.9950 - val_loss: 0.1785 - val_sparse_categorical_accuracy: 0.9500\nEpoch 51/60\n10/10 [==============================] - 1s 112ms/step - loss: 0.0174 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.1785 - val_sparse_categorical_accuracy: 0.9550\nEpoch 52/60\n10/10 [==============================] - 1s 110ms/step - loss: 0.0164 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.1795 - val_sparse_categorical_accuracy: 0.9550\nEpoch 53/60\n10/10 [==============================] - 1s 111ms/step - loss: 0.0147 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.1801 - val_sparse_categorical_accuracy: 0.9550\nEpoch 54/60\n10/10 [==============================] - 1s 112ms/step - loss: 0.0119 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.1806 - val_sparse_categorical_accuracy: 0.9550\nEpoch 55/60\n10/10 [==============================] - 1s 107ms/step - loss: 0.0118 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.1814 - val_sparse_categorical_accuracy: 0.9550\nEpoch 56/60\n10/10 [==============================] - 1s 109ms/step - loss: 0.0136 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.1830 - val_sparse_categorical_accuracy: 0.9550\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_accuracy = model.evaluate(test_ds_)"
      ],
      "metadata": {
        "tags": [],
        "cell_id": "fef3ed5eeb52419595d0a3001a60e6be",
        "source_hash": "25db6e28",
        "execution_start": 1671096721976,
        "execution_millis": 945,
        "deepnote_to_be_reexecuted": false,
        "deepnote_cell_type": "code",
        "id": "__63mh0McBQX",
        "outputId": "6a32eb53-d83b-475b-899d-14bcdc5f6bff"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "10/10 [==============================] - 1s 45ms/step - loss: 0.0203 - sparse_categorical_accuracy: 1.0000\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "test_accuracy = test_accuracy\n",
        "training_loss = history.history['loss'][-1]\n",
        "training_accuracy = history.history['sparse_categorical_accuracy'][-1]\n",
        "val_loss = history.history['val_loss'][-1]\n",
        "val_accuracy = history.history['val_sparse_categorical_accuracy'][-1]\n",
        "print(f'Training loss: {training_loss:.3f}')\n",
        "print(f'Training accuracy: {training_accuracy * 100:.2f}%')\n",
        "print(f'Validation loss: {val_loss:.3f}')\n",
        "print(f'Validation accuracy: {val_accuracy * 100:.2f}%')\n",
        "print(f'Test loss: {test_loss:.3f}')\n",
        "print(f'Test accuracy: {test_accuracy * 100:.2f}%')"
      ],
      "metadata": {
        "tags": [],
        "cell_id": "01627650b2524e70aaa3eef15f3a2245",
        "source_hash": "d854461c",
        "execution_start": 1671096722900,
        "execution_millis": 22,
        "deepnote_to_be_reexecuted": false,
        "deepnote_cell_type": "code",
        "id": "TnZSuHOvcBQY",
        "outputId": "daf5bf37-04be-481d-8d8a-7de589ed8717"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Training loss: 0.014\nTraining accuracy: 100.00%\nValidation loss: 0.183\nValidation accuracy: 95.50%\nTest loss: 0.020\nTest accuracy: 100.00%\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import tempfile\n",
        "\n",
        "\n",
        "def get_gzipped_model_size(file):\n",
        "  # It returns the size of the gzipped model in kilobytes.\n",
        "\n",
        "  _, zipped_file = tempfile.mkstemp('.zip')\n",
        "  with zipfile.ZipFile(zipped_file, 'w', compression=zipfile.ZIP_DEFLATED) as f:\n",
        "    f.write(file)\n",
        "\n",
        "  return os.path.getsize(zipped_file)/1000\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "tflite_model = converter.convert()\n",
        "model_file = '/work/tflite_models/15_dec_2022.tflite'\n",
        "# Save the model.\n",
        "with open(model_file, 'wb') as f:\n",
        "    f.write(tflite_model)\n",
        "\n",
        "print(\"Model Size: \", get_gzipped_model_size(model_file), ' KB')\n",
        "print(\"Model Size (no compression): \", os.path.getsize(model_file)/1000, ' KB')"
      ],
      "metadata": {
        "tags": [],
        "cell_id": "c6951ac65e7a47c99edee2b970030443",
        "source_hash": "4c416e01",
        "execution_start": 1671096722905,
        "execution_millis": 1821,
        "deepnote_to_be_reexecuted": false,
        "deepnote_cell_type": "code",
        "id": "6U_oaG6VcBQY",
        "outputId": "295b7c43-5a13-4ade-e9ec-c3152caf7969"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\nINFO:tensorflow:Assets written to: /tmp/tmph1_yz0_f/assets\nINFO:tensorflow:Assets written to: /tmp/tmph1_yz0_f/assets\nModel Size:  10.625  KB\nModel Size (no compression):  14.56  KB\n2022-12-15 09:32:04.524060: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:362] Ignored output_format.\n2022-12-15 09:32:04.524107: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:365] Ignored drop_control_dependency.\n2022-12-15 09:32:04.524826: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/tmph1_yz0_f\n2022-12-15 09:32:04.527023: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n2022-12-15 09:32:04.527047: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/tmph1_yz0_f\n2022-12-15 09:32:04.532171: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:354] MLIR V1 optimization pass is not enabled\n2022-12-15 09:32:04.533661: I tensorflow/cc/saved_model/loader.cc:229] Restoring SavedModel bundle.\n2022-12-15 09:32:04.598819: I tensorflow/cc/saved_model/loader.cc:213] Running initialization op on SavedModel bundle at path: /tmp/tmph1_yz0_f\n2022-12-15 09:32:04.610269: I tensorflow/cc/saved_model/loader.cc:305] SavedModel load for tags { serve }; Status: success: OK. Took 85455 microseconds.\n2022-12-15 09:32:04.635632: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "deepnote": {},
    "orig_nbformat": 2,
    "deepnote_notebook_id": "2a291bbbf328434aa0b88975e5dfbae3",
    "deepnote_persisted_session": {
      "createdAt": "2022-12-15T14:34:13.767Z"
    },
    "deepnote_execution_queue": [],
    "colab": {
      "provenance": []
    }
  }
}